{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e989e7e3-c5ee-4a50-a2b5-77bcadbc9851",
   "metadata": {},
   "source": [
    "# **Pipeline to Create Poem Dataset from Reddit's r/OCPoetry Thread**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69207df-4a03-4dc8-9532-cc91afec120b",
   "metadata": {},
   "source": [
    "Make sure to create a Reddit account and register for a project in order to use the Reddit API. Once that is done, note down your personal use script (credentials key) and secret key. Poems will be scraped from the r/OCPoetry subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3caef-f68c-4c5b-85bb-0a0bec6e5c1a",
   "metadata": {},
   "source": [
    "This is an academic project. In order to comply with Reddit API terms of service and guidelines, the actual dataset created will not be posted. However, you can create your own credentials if you want to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc50e4-8543-4f09-aae1-e1498272e29a",
   "metadata": {},
   "source": [
    "**Workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b885816-2163-45ef-9ba5-a3debb8b6cc1",
   "metadata": {},
   "source": [
    "1. **Data Collection:** PRAW + Reddit API to scrape 5k poems from r/OCPoetry. Fields like title, body, author to CSV/JSON\n",
    "2. **Preprocessing:** Remove spaces, special characters, fix formatting, normalize (lowercase)\n",
    "3. **Label Generation:** OpenAI API to generate symbolic labels for each poem, parse response and add labels as new column of dataset\n",
    "   - Using GPT 4.5\n",
    "4. **Dataset Finalization:** Structure with poem_text and labels and convert labels suitable for training, if needed\n",
    "5. **Model Training:** Choose pretrained transformer model and fine-tune it as multi-label classifier on created dataset (70/15/15)\n",
    "   - We need ground truth so we will use GPT 4.5 to train/label and then fine tune with GPT 3.5\n",
    "6. **Evaluation:** Eval metrics (precision, recall, F1, hamming loss) and human-evaluations comparing GPT 4.5 and 3.5's output\n",
    "7. **Deployment:** Input unseen poem, output predicted symbolic labels/themes\n",
    "8. **Iteration If Time:** Incorporate Wikidata SPARQL for entity linking and label enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c3ac8-8224-4b60-a38e-3645bb503a45",
   "metadata": {},
   "source": [
    "**TOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54fc021-e2f7-40f9-8ef6-95aaf52b6111",
   "metadata": {},
   "source": [
    "- Create a Reddit app to get client_id, client_secret, and user_agent\n",
    "- 60 requests per minute\n",
    "- Throttle requests accordingly to avoid temporary bans (no spamming)\n",
    "- No scraping personal info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968467ff-55e3-4b88-aa77-d8bcbf8b9f64",
   "metadata": {},
   "source": [
    "**Set up Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4be160c-e6ae-48df-856f-51dcc873979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (2.3.1)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.6.15)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from pandas) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marielle\\anaconda3\\envs\\tf_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: tqdm, update_checker, prawcore, praw\n",
      "\n",
      "   ------------------------------ --------- 3/4 [praw]\n",
      "   ---------------------------------------- 4/4 [praw]\n",
      "\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 tqdm-4.67.1 update_checker-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3a5ea-8668-4c39-a87f-0688a47622cc",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d054527-6afe-433e-92d5-2ca8ccbb6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from prawcore.exceptions import RequestException, ResponseException, ServerError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad9916-b622-45fb-a55e-5c5a37014b49",
   "metadata": {},
   "source": [
    "**Set Up Credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f320eafe-cb8e-4cb6-b4ce-e57cc2657a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id='bwe4o-iuFl22kqNIjpeqUg',\n",
    "    client_secret='N3Jm4lVrj_vZ3OiZf5x53eJ_HWtczw',\n",
    "    user_agent='symbolism-project-llm by /u/Mxrchives')\n",
    "\n",
    "# Choose the subreddit\n",
    "subreddit = reddit.subreddit('OCPoetry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb54b8-3ab8-4388-a7ba-c81f31139f90",
   "metadata": {},
   "source": [
    "**Test with 10 posts into a .csv first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb7ee29-90f3-47e7-b646-58e50dca33d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ocpoem_posts.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write header\n",
    "    writer.writerow(['title', 'author', 'poem_text'])\n",
    "\n",
    "    # 10 newest posts\n",
    "    for submission in subreddit.new(limit=10):\n",
    "        title = submission.title\n",
    "        author = str(submission.author)\n",
    "        poem_text = submission.selftext.replace('\\n', ' ')  # Replace newlines with spaces so that it's easier for cleaning later\n",
    "        writer.writerow([title, author, poem_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc91df9e-f5af-4b5b-9f33-dc840bd0523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8fe37f-0b54-4abf-8737-fcf71bcdf682",
   "metadata": {},
   "source": [
    "**Get 1K samples safely (make sure to not spam)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8822cf-d794-40b9-b28a-8857a957107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 50 posts, sleeping to respect rate limits...\n",
      "Fetched 100 posts, sleeping to respect rate limits...\n",
      "Fetched 150 posts, sleeping to respect rate limits...\n",
      "Fetched 200 posts, sleeping to respect rate limits...\n",
      "Fetched 250 posts, sleeping to respect rate limits...\n",
      "Fetched 300 posts, sleeping to respect rate limits...\n",
      "Fetched 350 posts, sleeping to respect rate limits...\n",
      "Fetched 400 posts, sleeping to respect rate limits...\n",
      "Fetched 450 posts, sleeping to respect rate limits...\n",
      "Fetched 500 posts, sleeping to respect rate limits...\n",
      "Fetched 550 posts, sleeping to respect rate limits...\n",
      "Fetched 600 posts, sleeping to respect rate limits...\n",
      "Fetched 650 posts, sleeping to respect rate limits...\n",
      "Fetched 700 posts, sleeping to respect rate limits...\n",
      "Fetched 750 posts, sleeping to respect rate limits...\n",
      "Fetched 800 posts, sleeping to respect rate limits...\n",
      "Fetched 850 posts, sleeping to respect rate limits...\n",
      "Fetched 900 posts, sleeping to respect rate limits...\n",
      "Fetched 950 posts, sleeping to respect rate limits...\n"
     ]
    }
   ],
   "source": [
    "with open('ocpoetry_posts.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['title', 'author', 'poem_text'])\n",
    "\n",
    "    count = 0\n",
    "    try:\n",
    "        for submission in subreddit.new(limit=1000):\n",
    "            # Write post info\n",
    "            writer.writerow([\n",
    "                submission.title,\n",
    "                submission.author.name if submission.author else 'deleted',\n",
    "                submission.selftext.replace('\\n', ' ')  # clean newlines\n",
    "            ])\n",
    "            count += 1\n",
    "\n",
    "            # pause every 50 posts for 2 seconds to be safe!!!\n",
    "            if count % 50 == 0:\n",
    "                print(f'Fetched {count} posts, sleeping to respect rate limits...')\n",
    "                time.sleep(2)\n",
    "\n",
    "    except (RequestException, ResponseException, ServerError) as e:\n",
    "        print(f'Request error: {e}')\n",
    "        print('Waiting for 10 seconds before retrying...')\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe0620-43cd-4c88-b983-47546a2005ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successful! Retrieving data took only 1 minute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f1d0db-0ac0-4e3f-a646-ed3c8a3f1c2a",
   "metadata": {},
   "source": [
    "**Note:** 990 posts were retrieved! Still good for our data so no worries. Sometimes this happens because .new() or .limit parameters don’t return the exact number requested. This can be due to deleted posts. We cannot retrieve those last remaining 10 posts specifically to get to 1k because Reddit API does not let you randomly fetch. It will give you the newest posts in descending order. This is still okay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b01728-7abd-40e3-8ad6-d60ba439b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use the curated ocpoetry_posts.csv to head to the data cleaning process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
